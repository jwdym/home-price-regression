{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Price Prediction\n\nThis notebook goes over the creation of a simple model training to prediction pipeline. There isn't much in the ways of EDA or feature engineering, and there are many details about the data set that will slip by (for instance treating `MoSold` as a numerical variable rather than a cateogircal). There's is also no hyperparameter tuning or any sort of model tuning used in this notebook, and the model evaluation is limited to just basic metrics.\n\n### Steps\n1. Transform the data\n2. Handle missing values\n3. Train model\n4. Make predictions","metadata":{}},{"cell_type":"code","source":"import os\nimport sklearn\nimport scipy\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:41:29.995697Z","iopub.execute_input":"2021-05-20T00:41:29.996015Z","iopub.status.idle":"2021-05-20T00:41:31.145994Z","shell.execute_reply.started":"2021-05-20T00:41:29.995945Z","shell.execute_reply":"2021-05-20T00:41:31.145241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform the Data\n\nFirst we need to read in the data to transform it to be model ready. This involves converting categorical variables such as `Utilities` and `LotShape` into one hot encoded vectors that can be read by a model. First things first we'll read in the data set from our local directory.","metadata":{}},{"cell_type":"code","source":"dat = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:41:48.652057Z","iopub.execute_input":"2021-05-20T00:41:48.652444Z","iopub.status.idle":"2021-05-20T00:41:48.695276Z","shell.execute_reply.started":"2021-05-20T00:41:48.652409Z","shell.execute_reply":"2021-05-20T00:41:48.694451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dat = dat.drop('Id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:41:52.803628Z","iopub.execute_input":"2021-05-20T00:41:52.803968Z","iopub.status.idle":"2021-05-20T00:41:52.821661Z","shell.execute_reply.started":"2021-05-20T00:41:52.803943Z","shell.execute_reply":"2021-05-20T00:41:52.820538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dat.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:41:55.040016Z","iopub.execute_input":"2021-05-20T00:41:55.040406Z","iopub.status.idle":"2021-05-20T00:41:55.072016Z","shell.execute_reply.started":"2021-05-20T00:41:55.040373Z","shell.execute_reply":"2021-05-20T00:41:55.071178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the data set loaded we need to identify the categorical data from the numeric data, as the categorical data will need to be transformed in some manner before prediction (It's good practice to transform the numeric data too, either to standardize it or for identifying outliers).","metadata":{}},{"cell_type":"code","source":"# Identify numerical data type columns\nnum_cols = list(dat.columns[dat.dtypes == np.int64])\nnum_cols += list(dat.columns[dat.dtypes == np.float64])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:41:58.367694Z","iopub.execute_input":"2021-05-20T00:41:58.368296Z","iopub.status.idle":"2021-05-20T00:41:58.374025Z","shell.execute_reply.started":"2021-05-20T00:41:58.368236Z","shell.execute_reply":"2021-05-20T00:41:58.373344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify categorical data type columns\ncat_cols = list(dat.columns[dat.dtypes == object])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:02.479938Z","iopub.execute_input":"2021-05-20T00:42:02.480306Z","iopub.status.idle":"2021-05-20T00:42:02.484919Z","shell.execute_reply.started":"2021-05-20T00:42:02.480265Z","shell.execute_reply":"2021-05-20T00:42:02.483921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handle Missing Values\nWith our categorical and numeric data seperated, we need to address the issue of missing values within the data set. We'll start by looking at what variables have missing data and how much.","metadata":{}},{"cell_type":"code","source":"# Quantify amount of missing data\nnull_values = (np.sum(dat.isna()))[np.sum(dat.isna()) > 0]\nprop = (np.sum(dat.isna()) / dat.shape[0])[np.sum(dat.isna()) > 0] * 100\nmissing_data = pd.concat([null_values, prop], axis=1, keys=['Total', 'Percentage'])\nmissing_data.sort_values(by='Total', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:05.322782Z","iopub.execute_input":"2021-05-20T00:42:05.323321Z","iopub.status.idle":"2021-05-20T00:42:05.386155Z","shell.execute_reply.started":"2021-05-20T00:42:05.323275Z","shell.execute_reply":"2021-05-20T00:42:05.385203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variables such as `PoolQC` or `MiscFeature` are more missing values than they are observed values for whatever reason. We can impute missing data using a variety of techniques, in this case we'll drop variables with a large amount of missing data as they don't provide much information.","metadata":{}},{"cell_type":"code","source":"drops = ['Alley', 'PoolQC', 'Fence', 'FireplaceQu', 'MiscFeature', 'SalePrice']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:09.657773Z","iopub.execute_input":"2021-05-20T00:42:09.658089Z","iopub.status.idle":"2021-05-20T00:42:09.662604Z","shell.execute_reply.started":"2021-05-20T00:42:09.658062Z","shell.execute_reply":"2021-05-20T00:42:09.661644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = list(set(cat_cols) - set(drops))\nnum_cols = list(set(num_cols) - set(drops))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:12.267884Z","iopub.execute_input":"2021-05-20T00:42:12.268191Z","iopub.status.idle":"2021-05-20T00:42:12.272728Z","shell.execute_reply.started":"2021-05-20T00:42:12.268165Z","shell.execute_reply":"2021-05-20T00:42:12.271821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_categories = []\nfor c in cat_cols:\n    cat_categories.append(list(set(dat[c])))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:14.594035Z","iopub.execute_input":"2021-05-20T00:42:14.59439Z","iopub.status.idle":"2021-05-20T00:42:14.606144Z","shell.execute_reply.started":"2021-05-20T00:42:14.594359Z","shell.execute_reply":"2021-05-20T00:42:14.604981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After dropping variables that were missing a substantial amount of data we can go on to encode the categorical variables via one hot encoding.","metadata":{}},{"cell_type":"code","source":"enc = OneHotEncoder(categories=cat_categories, handle_unknown='ignore')\nenc.fit(dat[cat_cols])\nenc.transform(dat[cat_cols]).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:17.232418Z","iopub.execute_input":"2021-05-20T00:42:17.232763Z","iopub.status.idle":"2021-05-20T00:42:17.276841Z","shell.execute_reply.started":"2021-05-20T00:42:17.232723Z","shell.execute_reply":"2021-05-20T00:42:17.276223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the data encoded and the encoder fit to the training data it's time to impute the missing values of the data set. We'll use KNN imputation to fill in any missing values in the data set.","metadata":{}},{"cell_type":"code","source":"dat_imp = pd.concat([pd.DataFrame(enc.transform(dat[cat_cols]).toarray()), dat[num_cols]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:23.715512Z","iopub.execute_input":"2021-05-20T00:42:23.716081Z","iopub.status.idle":"2021-05-20T00:42:23.758516Z","shell.execute_reply.started":"2021-05-20T00:42:23.716048Z","shell.execute_reply":"2021-05-20T00:42:23.757566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=5)\nx_train = imputer.fit_transform(dat_imp)\ny_train = dat.SalePrice","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:27.085516Z","iopub.execute_input":"2021-05-20T00:42:27.085847Z","iopub.status.idle":"2021-05-20T00:42:27.44435Z","shell.execute_reply.started":"2021-05-20T00:42:27.085818Z","shell.execute_reply":"2021-05-20T00:42:27.443287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model\n\nThere is usually a bit more to training the model than just passing a training data set and letting it build, however the default model works alright in this case so we'll train a gradient boosted regressor model on our newly transformed and imputed data set.","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingRegressor()\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:30.523374Z","iopub.execute_input":"2021-05-20T00:42:30.523709Z","iopub.status.idle":"2021-05-20T00:42:31.496768Z","shell.execute_reply.started":"2021-05-20T00:42:30.52368Z","shell.execute_reply":"2021-05-20T00:42:31.495878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_train, model.predict(x_train))\nplt.xlabel('Observed Prices')\nplt.ylabel('Predicted Prices')\nplt.title('Observed vs Predicted Home Sale Prices')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:35.581011Z","iopub.execute_input":"2021-05-20T00:42:35.581358Z","iopub.status.idle":"2021-05-20T00:42:35.793271Z","shell.execute_reply.started":"2021-05-20T00:42:35.581329Z","shell.execute_reply":"2021-05-20T00:42:35.792249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:38.517954Z","iopub.execute_input":"2021-05-20T00:42:38.51837Z","iopub.status.idle":"2021-05-20T00:42:38.529775Z","shell.execute_reply.started":"2021-05-20T00:42:38.518334Z","shell.execute_reply":"2021-05-20T00:42:38.528605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall the model doesn't appear to have done too poorly in it's task of predicting home prices, though these metrics are based on the training set so they should be good. With the model trained and making predictions let's move onto making predictions on the test data.","metadata":{}},{"cell_type":"markdown","source":"# Making Predictions\n\nThe test data set needs to be prepared just as the training data set was, and the data needs to be in the exact same format as the training data was in order to properly operate. We'll use the encoder as well as the imputer trained above in order to format the test data.","metadata":{}},{"cell_type":"code","source":"test_dat = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:41.530376Z","iopub.execute_input":"2021-05-20T00:42:41.530876Z","iopub.status.idle":"2021-05-20T00:42:41.572001Z","shell.execute_reply.started":"2021-05-20T00:42:41.530846Z","shell.execute_reply":"2021-05-20T00:42:41.571303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc.transform(test_dat[cat_cols]).toarray()\ntest_imp = pd.concat([pd.DataFrame(enc.transform(test_dat[cat_cols]).toarray()), test_dat[num_cols]], axis=1)\nx_test = imputer.transform(test_imp)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:44.299036Z","iopub.execute_input":"2021-05-20T00:42:44.299615Z","iopub.status.idle":"2021-05-20T00:42:44.690831Z","shell.execute_reply.started":"2021-05-20T00:42:44.29958Z","shell.execute_reply":"2021-05-20T00:42:44.689779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the test data formatted we can now generate the output for Kaggle submissions, to do this we'll use pandas to write out a dataframe with the `Id` and corresponding `SalePrice` predictions.","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'Id': test_dat.Id, 'SalePrice': model.predict(x_test)})","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:46.849134Z","iopub.execute_input":"2021-05-20T00:42:46.849486Z","iopub.status.idle":"2021-05-20T00:42:46.859384Z","shell.execute_reply.started":"2021-05-20T00:42:46.849458Z","shell.execute_reply":"2021-05-20T00:42:46.85857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T00:42:54.68207Z","iopub.execute_input":"2021-05-20T00:42:54.682406Z","iopub.status.idle":"2021-05-20T00:42:54.691243Z","shell.execute_reply.started":"2021-05-20T00:42:54.682378Z","shell.execute_reply":"2021-05-20T00:42:54.690393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output.to_csv('my_submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}